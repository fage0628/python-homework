{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Purchases</th>\n",
       "      <th>B01001001</th>\n",
       "      <th>B01001002</th>\n",
       "      <th>B01001003</th>\n",
       "      <th>B01001004</th>\n",
       "      <th>B01001005</th>\n",
       "      <th>B01001006</th>\n",
       "      <th>B01001007</th>\n",
       "      <th>B01001008</th>\n",
       "      <th>B01001009</th>\n",
       "      <th>...</th>\n",
       "      <th>B19001008</th>\n",
       "      <th>B19001009</th>\n",
       "      <th>B19001010</th>\n",
       "      <th>B19001011</th>\n",
       "      <th>B19001012</th>\n",
       "      <th>B19001013</th>\n",
       "      <th>B19001014</th>\n",
       "      <th>B19001015</th>\n",
       "      <th>B19001016</th>\n",
       "      <th>B19001017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>206252</td>\n",
       "      <td>469.226965</td>\n",
       "      <td>31.432422</td>\n",
       "      <td>35.219052</td>\n",
       "      <td>33.628765</td>\n",
       "      <td>20.121017</td>\n",
       "      <td>12.610787</td>\n",
       "      <td>6.734480</td>\n",
       "      <td>6.225394</td>\n",
       "      <td>...</td>\n",
       "      <td>49.409690</td>\n",
       "      <td>53.306757</td>\n",
       "      <td>42.318307</td>\n",
       "      <td>83.167229</td>\n",
       "      <td>89.249208</td>\n",
       "      <td>102.141470</td>\n",
       "      <td>52.872330</td>\n",
       "      <td>36.440765</td>\n",
       "      <td>23.446284</td>\n",
       "      <td>21.197485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>61399</td>\n",
       "      <td>486.538869</td>\n",
       "      <td>22.899396</td>\n",
       "      <td>21.531295</td>\n",
       "      <td>27.036271</td>\n",
       "      <td>16.808091</td>\n",
       "      <td>28.355511</td>\n",
       "      <td>18.192479</td>\n",
       "      <td>13.534422</td>\n",
       "      <td>...</td>\n",
       "      <td>59.231680</td>\n",
       "      <td>50.093078</td>\n",
       "      <td>40.700626</td>\n",
       "      <td>92.612963</td>\n",
       "      <td>117.363344</td>\n",
       "      <td>113.344051</td>\n",
       "      <td>75.774243</td>\n",
       "      <td>33.000508</td>\n",
       "      <td>33.169741</td>\n",
       "      <td>24.792689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>73170</td>\n",
       "      <td>489.859232</td>\n",
       "      <td>28.905289</td>\n",
       "      <td>36.271696</td>\n",
       "      <td>28.235616</td>\n",
       "      <td>21.566216</td>\n",
       "      <td>12.218122</td>\n",
       "      <td>7.243406</td>\n",
       "      <td>7.380074</td>\n",
       "      <td>...</td>\n",
       "      <td>63.996993</td>\n",
       "      <td>47.322923</td>\n",
       "      <td>42.505211</td>\n",
       "      <td>70.420610</td>\n",
       "      <td>90.033143</td>\n",
       "      <td>98.677692</td>\n",
       "      <td>54.703249</td>\n",
       "      <td>20.125056</td>\n",
       "      <td>11.890525</td>\n",
       "      <td>16.537397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>251724</td>\n",
       "      <td>505.585483</td>\n",
       "      <td>32.054949</td>\n",
       "      <td>31.757004</td>\n",
       "      <td>28.102207</td>\n",
       "      <td>18.651380</td>\n",
       "      <td>12.080692</td>\n",
       "      <td>7.035483</td>\n",
       "      <td>7.686991</td>\n",
       "      <td>...</td>\n",
       "      <td>54.790900</td>\n",
       "      <td>48.681562</td>\n",
       "      <td>43.873381</td>\n",
       "      <td>84.717507</td>\n",
       "      <td>112.204444</td>\n",
       "      <td>127.137252</td>\n",
       "      <td>83.019904</td>\n",
       "      <td>43.731067</td>\n",
       "      <td>38.851729</td>\n",
       "      <td>40.427349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>37382</td>\n",
       "      <td>495.586111</td>\n",
       "      <td>25.413301</td>\n",
       "      <td>29.318924</td>\n",
       "      <td>26.162324</td>\n",
       "      <td>19.260607</td>\n",
       "      <td>12.893906</td>\n",
       "      <td>6.580707</td>\n",
       "      <td>7.062222</td>\n",
       "      <td>...</td>\n",
       "      <td>58.883378</td>\n",
       "      <td>51.761414</td>\n",
       "      <td>47.310187</td>\n",
       "      <td>81.902582</td>\n",
       "      <td>93.793717</td>\n",
       "      <td>130.103014</td>\n",
       "      <td>71.982704</td>\n",
       "      <td>36.118530</td>\n",
       "      <td>31.603714</td>\n",
       "      <td>19.648989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>230108</td>\n",
       "      <td>499.217759</td>\n",
       "      <td>32.767222</td>\n",
       "      <td>35.465955</td>\n",
       "      <td>36.169972</td>\n",
       "      <td>22.107011</td>\n",
       "      <td>13.593617</td>\n",
       "      <td>5.671250</td>\n",
       "      <td>6.144941</td>\n",
       "      <td>...</td>\n",
       "      <td>46.201509</td>\n",
       "      <td>46.504580</td>\n",
       "      <td>47.750539</td>\n",
       "      <td>90.247845</td>\n",
       "      <td>122.294810</td>\n",
       "      <td>160.998114</td>\n",
       "      <td>100.372665</td>\n",
       "      <td>56.045708</td>\n",
       "      <td>39.017601</td>\n",
       "      <td>28.859106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>928405</td>\n",
       "      <td>493.339652</td>\n",
       "      <td>28.576968</td>\n",
       "      <td>30.484541</td>\n",
       "      <td>33.556476</td>\n",
       "      <td>21.172872</td>\n",
       "      <td>15.826067</td>\n",
       "      <td>7.983585</td>\n",
       "      <td>7.764930</td>\n",
       "      <td>...</td>\n",
       "      <td>37.631856</td>\n",
       "      <td>38.216597</td>\n",
       "      <td>35.938423</td>\n",
       "      <td>72.751062</td>\n",
       "      <td>98.274145</td>\n",
       "      <td>132.154395</td>\n",
       "      <td>102.318122</td>\n",
       "      <td>66.729965</td>\n",
       "      <td>71.766845</td>\n",
       "      <td>60.436761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>15890</td>\n",
       "      <td>498.237886</td>\n",
       "      <td>32.473254</td>\n",
       "      <td>34.675897</td>\n",
       "      <td>36.752675</td>\n",
       "      <td>26.557583</td>\n",
       "      <td>17.747011</td>\n",
       "      <td>4.845815</td>\n",
       "      <td>16.362492</td>\n",
       "      <td>...</td>\n",
       "      <td>32.177704</td>\n",
       "      <td>30.309321</td>\n",
       "      <td>17.023043</td>\n",
       "      <td>15.569857</td>\n",
       "      <td>53.975503</td>\n",
       "      <td>12.455885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>24327</td>\n",
       "      <td>475.562133</td>\n",
       "      <td>30.007810</td>\n",
       "      <td>29.514531</td>\n",
       "      <td>31.158795</td>\n",
       "      <td>18.950138</td>\n",
       "      <td>27.335882</td>\n",
       "      <td>13.318535</td>\n",
       "      <td>16.771488</td>\n",
       "      <td>...</td>\n",
       "      <td>46.472856</td>\n",
       "      <td>53.871001</td>\n",
       "      <td>26.675003</td>\n",
       "      <td>64.707721</td>\n",
       "      <td>87.110555</td>\n",
       "      <td>108.679796</td>\n",
       "      <td>64.916120</td>\n",
       "      <td>40.637699</td>\n",
       "      <td>16.359279</td>\n",
       "      <td>10.003126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2861</td>\n",
       "      <td>19979950</td>\n",
       "      <td>483.276184</td>\n",
       "      <td>31.650830</td>\n",
       "      <td>30.824702</td>\n",
       "      <td>31.484864</td>\n",
       "      <td>19.676576</td>\n",
       "      <td>12.867249</td>\n",
       "      <td>6.957375</td>\n",
       "      <td>6.725342</td>\n",
       "      <td>...</td>\n",
       "      <td>36.342018</td>\n",
       "      <td>36.246108</td>\n",
       "      <td>31.909623</td>\n",
       "      <td>64.438383</td>\n",
       "      <td>87.488833</td>\n",
       "      <td>117.364952</td>\n",
       "      <td>93.519131</td>\n",
       "      <td>63.997025</td>\n",
       "      <td>80.800359</td>\n",
       "      <td>102.752258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Purchases  B01001001   B01001002  B01001003  B01001004  B01001005  \\\n",
       "0           22     206252  469.226965  31.432422  35.219052  33.628765   \n",
       "1            7      61399  486.538869  22.899396  21.531295  27.036271   \n",
       "2            3      73170  489.859232  28.905289  36.271696  28.235616   \n",
       "3           94     251724  505.585483  32.054949  31.757004  28.102207   \n",
       "4            0      37382  495.586111  25.413301  29.318924  26.162324   \n",
       "5           19     230108  499.217759  32.767222  35.465955  36.169972   \n",
       "6           11     928405  493.339652  28.576968  30.484541  33.556476   \n",
       "7            2      15890  498.237886  32.473254  34.675897  36.752675   \n",
       "8            0      24327  475.562133  30.007810  29.514531  31.158795   \n",
       "9         2861   19979950  483.276184  31.650830  30.824702  31.484864   \n",
       "\n",
       "   B01001006  B01001007  B01001008  B01001009     ...      B19001008  \\\n",
       "0  20.121017  12.610787   6.734480   6.225394     ...      49.409690   \n",
       "1  16.808091  28.355511  18.192479  13.534422     ...      59.231680   \n",
       "2  21.566216  12.218122   7.243406   7.380074     ...      63.996993   \n",
       "3  18.651380  12.080692   7.035483   7.686991     ...      54.790900   \n",
       "4  19.260607  12.893906   6.580707   7.062222     ...      58.883378   \n",
       "5  22.107011  13.593617   5.671250   6.144941     ...      46.201509   \n",
       "6  21.172872  15.826067   7.983585   7.764930     ...      37.631856   \n",
       "7  26.557583  17.747011   4.845815  16.362492     ...      32.177704   \n",
       "8  18.950138  27.335882  13.318535  16.771488     ...      46.472856   \n",
       "9  19.676576  12.867249   6.957375   6.725342     ...      36.342018   \n",
       "\n",
       "   B19001009  B19001010  B19001011   B19001012   B19001013   B19001014  \\\n",
       "0  53.306757  42.318307  83.167229   89.249208  102.141470   52.872330   \n",
       "1  50.093078  40.700626  92.612963  117.363344  113.344051   75.774243   \n",
       "2  47.322923  42.505211  70.420610   90.033143   98.677692   54.703249   \n",
       "3  48.681562  43.873381  84.717507  112.204444  127.137252   83.019904   \n",
       "4  51.761414  47.310187  81.902582   93.793717  130.103014   71.982704   \n",
       "5  46.504580  47.750539  90.247845  122.294810  160.998114  100.372665   \n",
       "6  38.216597  35.938423  72.751062   98.274145  132.154395  102.318122   \n",
       "7  30.309321  17.023043  15.569857   53.975503   12.455885    0.000000   \n",
       "8  53.871001  26.675003  64.707721   87.110555  108.679796   64.916120   \n",
       "9  36.246108  31.909623  64.438383   87.488833  117.364952   93.519131   \n",
       "\n",
       "   B19001015  B19001016   B19001017  \n",
       "0  36.440765  23.446284   21.197485  \n",
       "1  33.000508  33.169741   24.792689  \n",
       "2  20.125056  11.890525   16.537397  \n",
       "3  43.731067  38.851729   40.427349  \n",
       "4  36.118530  31.603714   19.648989  \n",
       "5  56.045708  39.017601   28.859106  \n",
       "6  66.729965  71.766845   60.436761  \n",
       "7   0.000000   0.000000    0.000000  \n",
       "8  40.637699  16.359279   10.003126  \n",
       "9  63.997025  80.800359  102.752258  \n",
       "\n",
       "[10 rows x 190 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRaw = pd.read_csv(\"finalmaster-ratios.csv\")\n",
    "dfRaw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat a list\n",
    "allvariablenames = list(dfRaw.columns.values)\n",
    "#check the list\n",
    "allvariablenames\n",
    "#delete first 7 variable\n",
    "allvariablenames = allvariablenames[8:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load predictors into dataframe\n",
    "predictors = dfRaw[allvariablenames]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load target into dataframe\n",
    "target = dfRaw['# Purchases'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets, with 30% retained for test(seems like write a function.we need to use 30% to test)\n",
    "\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Every Name from above\n",
    "#(1) pred_train, the predictors training set    \n",
    "#(2) pred_test, the predictors test test\n",
    "#(3) target_train, the target training set and \n",
    "#(4) tar_test, the target test set. We'll feed these datasets to our model in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.739e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.736e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.579e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.483e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.383e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=5.383e-01, previous alpha=5.383e-01, with an active set of 17 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.457e-02, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=3.414e-02, with an active set of 94 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=3.106e-02, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.022e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.022e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 134 iterations, alpha=3.018e-02, previous alpha=2.946e-02, with an active set of 101 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.200e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.188e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=7.036e-01, previous alpha=6.809e-01, with an active set of 18 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.916e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.645e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.456e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.710e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.710e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.614e-01, with an active set of 44 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.613e-01, with an active set of 44 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.477e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.443e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.317e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.317e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=1.285e-01, previous alpha=1.266e-01, with an active set of 60 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.368e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.535e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.535e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.298e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.144e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=7.357e-01, previous alpha=6.074e-01, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.442e+00, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.333e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.051e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.193e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.572e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.446e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.382e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.997e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.777e-01, previous alpha=1.768e-01, with an active set of 35 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.554e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.277e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.089e-01, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.880e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=1.742e-01, previous alpha=1.733e-01, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/yifanli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model=LassoLarsCV(precompute=False, cv=10).fit(pred_train,tar_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01001036' 2.78613659551325]\n",
      "['B01001037' 0.9200572652790081]\n",
      "['B01001038' 0.9459340522644344]\n",
      "['B02001005' 0.39156809216155536]\n",
      "['B13014026' 0.22056164158451882]\n",
      "['B13014027' 0.05049787197081054]\n",
      "['B19001017' 1.6062678580473928]\n"
     ]
    }
   ],
   "source": [
    "predictors_model=pd.DataFrame(allvariablenames)\n",
    "predictors_model.columns = ['label']\n",
    "predictors_model['coeff'] = model.coef_\n",
    "\n",
    "for index, row in predictors_model.iterrows():\n",
    "    if row['coeff'] > 0:\n",
    "        print(row.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#QUESTION #1: In your own words, explain what the above lines of code are doing. Why am I doing it? Explain each line.\n",
    "#Answer:First, assign allvariablenames equal to predictors_model, and name predictors_model.columns to 'Label'. \n",
    "#And then, add coefficient to each row, and name them 'model.coef_'. Lastly, we should find which coefficient > 0, if the coeff >0, print it\n",
    "# Because any coefficients that are non-zero are significant, and thus variables that truly predict unique amounts of sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION #2: Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically?\n",
    "# ['B01001014' 0.8557908775529921] Males aged 40 to 44 Years.\n",
    "#\"In areas where there are more Males aged 40-44, we sell more 0.8557908775529921 unit Bobo Bars.\"\n",
    "\n",
    "# ['B01001036' 2.505392496591849] Females aged 30 to 34 Years.\n",
    "#In areas where there are more Females aged 30-34, we sell more 2.505392496591849 unit Bobo Bars.\n",
    "\n",
    "# ['B01001037' 0.8894214357013622] Females aged 35 to 39 Years.\n",
    "#In areas where there are more Females aged 35-39, we sell more 0.8894214357013622 unit Bobo Bars.\n",
    "\n",
    "# ['B01001038' 1.5315839680821497] Females aged 40 to 44 Years.\n",
    "#In areas where there are more Females aged 40-44, we sell more 1.5315839680821497 unit Bobo Bars.\n",
    "\n",
    "# ['B02001005' 0.4125408937426837] Asian Alone\n",
    "#In areas where there are asian alone, we sell 0.4125408937426837 unit Bobo Bars.\n",
    "\n",
    "# ['B13014026' 0.4800240326923769] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree\n",
    "#In areas where there are more females aged 15-50 years Who Had a Birth in the Past 12 Months with Bachelor's Degree, \n",
    "#we sell more 0.4800240326923769 unit Bobo Bars.\n",
    "\n",
    "# ['B13014027' 0.6977454940063235] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree\n",
    "#In areas where there are more female aged 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree, \n",
    "#we sell more 0.6977454940063235 unit Bobo Bars.\n",
    "\n",
    "# ['B13016001' 874922971.7249781] Women 15 to 50 Years Who Had a Birth in the Past 12 Months\n",
    "#In areas where there are more females aged 15-50 Years Who Had a Birth in the Past 12 Months, we sell more 874922971.7249781 unit Bobo Bars.\n",
    "\n",
    "# ['B19001017' 1.4834465563617387] Household with income $200,000 or More\n",
    "#In areas where there are Household with income $200,000 or More, we sell more 1.4834465563617387 unit Bobo Bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#QUESTION #3: If I had to report only two census variables to my boss that most steeply predicted sales, \n",
    "# what would those be?\n",
    "#Answer:If the value of coeff is higher, it will be more stteep between sales and the variable. \n",
    "# I will select B13016001 and B01001036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data MSE\n",
      "22528.486826258624\n",
      "training data MSE\n",
      "41578.280293705764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "print ('training data MSE')\n",
    "print(train_error)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "train_test = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('training data MSE')\n",
    "print(train_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION #4: Are the training and text set mean squared errors similar? \n",
    "#What does that mean practically? \n",
    "#Answer: No, they are not similar. \n",
    "#“In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, \n",
    "#the average squared difference between the estimated values and what is estimated.”（Wikipedia）\n",
    "#\"The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) (or sometimes root-mean-squared error) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.\"\n",
    "#(Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data R-square\n",
      "0.22266652028942102\n",
      "testing data R-square\n",
      "0.17529294561525344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "print ('training data R-square')\n",
    "print(rsquared_train)\n",
    "rsquared_test=model.score(pred_test,tar_test)\n",
    "print ('testing data R-square')\n",
    "print(rsquared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training data R_square is better than testing data R-square.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION #5: If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?\n",
    "#Answer: This is not a great method to predict sales. Because census data cannot reflect the result very well. \n",
    "#To be specific, training data MSE is huge, but R-square is very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y interecept:\n",
      "2.758738710322291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"y interecept:\")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION #6: What is our baseline sales number? What does that mean, practically? \n",
    "#Think back to what y-intercepts mean in regression models.\n",
    "#The y intercept is 22.1967, and the baseline sales number is 22.1967."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
